<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
        integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/svg+xml" href="./img_cmivqa/logo2.svg" sizes="any">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
<!-- 插入公式 -->
</script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BKN0PM1J6W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-BKN0PM1J6W');
    </script>
    <title>Multilingual Medical Instructional Video Question Answering Challenge</title>
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="nav">
        <div class="container"><a href="/" class="navbar-brand">MMIVQA</a>
            <ul class="navbar-nav mr-auto order-1">
                <li class="nav-item"><a class="nav-link" href="#challenge">Overview</a></li>
                <li class="nav-item"><a class="nav-link" href="#dataset">Dataset</a></li>
                <li class="nav-item"><a class="nav-link" href="#dates">Dates</a></li>
                <li class="nav-item"><a class="nav-link" href="#award">Award</a></li>
                <li class="nav-item"><a class="nav-link" href="#organizers">Organizers</a></li>
                <li class="nav-item"><a class="nav-link" href="#reference">Reference</a></li>
                <li class="nav-item"><a class="nav-link" href="./leaderboard.html">LeaderboardA</a></li>
                <li class="nav-item"><a class="nav-link" href="./leaderboardb.html">LeaderboardB</a></li>
<!--                TODO leaderboard-->
            </ul>
            <ul class="navbar-nav ml-auto order-1">
<!--                <li class="nav-item"><a class="nav-link" href="https://project.mhzhou.com/vico/">Full Dataset</a></li>-->
<!--                <li class="nav-item"><a class="nav-link" href="https://arxiv.org/abs/2112.13548">Paper</a></li>-->
                <li class="nav-item"><a class="nav-link" href="https://github.com/cmivqa/NLPCC-2023-Shared-Task-5">Code</a></li>
                <li class="nav-item"><a class="nav-link" href="https://github.com/WENGSYX/CMIVQA_Baseline">Baseline</a></li>
            </ul>
        </div>
        </div>
    </nav>

    <main id="main" class="site-main main">
        <section class="container" id="banner">
            <img src="./img_cmivqa/logo_mmivqa.svg" width=150>
            <div class="title-text-box">
                 <h1 class="display-4 text-bold">MMIVQA</h1>
                <h2 class="display-5 text-bold">Multilingual Medical Instructional Video Question Answering Challenge</h2>
                <h5 class="text-regular">May held in ACM MULTIMEDIA 2024</h5>
            </div>
            <a class="btn" id="learn-more-btn" href="#challenge" role="button">Learn more</a>
            <a class="btn" id="download-btn" href="#downloads" role="button">Download dataset</a>
        </section>

        <section class="container">
            <div class="card cardx" id="challenge">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/prize.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Challenge Overview</h5>
                          Chinese Medical Instructional Video Question Answering Challenge
                        </div>
                      </div>
                    <p class="card-text">
                        Recently, the advent of online videos has revolutionized the way in which information or knowledge is obtained [1]. Many people find instructional videos an effective and efficient way to teach or learn how to complete a specific task through a series of step-by-step procedures [2]. In this context, a new task called Temporal Answering Grounding in Video (TAGV) is proposed to find the video frame span (visual answer) corresponding to the input question. However, current TAGV dataset was created for a single language (i.e., English or Chinese), it is also important to include the development of multilingual applications. To promote research on TAGV, we propose the Multilingual Medical Instructional Video Question Answering (MMIVQA) challenge. The datasets for this challenge contain high-quality Chinese and English medical instructional videos, with manual annotations provided by medical experts. The MMIVQA challenge includes three tracks, namely Track 1, Multilingual Temporal Answering Grounding in Single Video (mTAGSV), Track 2 Multilingual Video Corpus Retrieval, mVCR) and multilingual Temporal Answering Grounding in Video Corpus (mTAGVC). The ultimate goal of this joint task is to develop a system that can provide multilingual Q&A function with moment-to-moment video clips for first aid, medical emergencies or medical education.
                    </p>
                    <p class="card-text">This shared task includes three tracks: Multilingual Temporal Answer Grounding in Singe Video (mTAGSV), Multilingual Video Corpus Retrieval (mVCR) and Multilingual Temporal Answer Grounding in Video Corpus (mTAGVC).

                    <center><img src="./img_cmivqa/track1.png" width=90%></center>
                    <div>
<center><h6><br>Fig. 1: Illustration of Multilingual Temporal Answer Grounding in Singe Video (mTAGSV). </h6></center>
                        <ul>
</div>
                        <li ><b>Track 1. Multilingual Temporal Answer Grounding in Singe Video (mTAGSV): As shown in Fig. 1: given a medical or health-related question and a single untrimmed bilingual medical instructional video, this track aims to locate the temporal answer (start and end time points) within the video. We also provide both Chinese and English questions and captions for promote the multilingual methods.</li><!--li>Given the identity and audio signals of the speaker, participants are challenged to develop an algorithm to generate a video that can match the audio signals.</li-->
                </ul>
                    <center><img src="./img_cmivqa/track2.png" width=90% style="margin-top: 15px"></center>
                    <div>
<center><h6><br>Fig. 2: Multilingual Illustration of Video Corpus Retrieval (mVCR). </h6></center>
</div>
                    <ul>
                    <li><b>Track 2. Multilingual Video Corpus Retrieval (mVCR): As shown in Fig. 2, given a medical or health-related question and a large collection of untrimmed bilingual medical instructional videos, this track aims to find the most relevant video corresponding to the given question in the video corpus.</li-->
                   </ul>
                        <center> <img src="./img_cmivqa/track3.png" width=90% style="margin-top: 15px"></center>
                    <div>
<center><h6><br>Fig. 3: Illustration of Multilingual Temporal Answer Grounding in Video Corpus (mTAGVC).</h6></center>
<ul>
                    <li><b>Track 3. Multilingual Temporal Answer Grounding in Video Corpus (mTAGVC): As shown in Fig. 3, given a text question and a large collection of untrimmed bilingual medical instructional videos, this track aims at finding the matching video answer span within the most relevant video corresponding to the given question in the video corpus.</li-->
                </ul>
                    </p>
                    <!--
                    <b>Vivid talking head video generation</b> conditioned on the identity and audio signals of the speaker; <b>Responsive listening head video generation</b> conditioned on the identity of the listener and with responding to the speaker’s behaviors in real-time. The main difficulties lie in the large amount of products labels, and fine-grained details of similar products. Top-1 and Top-5 accuracy is measured to rank each submission.
                    </p>
                    -->
                    <p class="card-text">
                        The team constitution (members of a team) cannot be changed after the evaluation period has begun. Individuals and teams with top submissions will present their work at the workshop. We also encourage every team to upload a paper (See <a href="http://tcci.ccf.org.cn/conference/2023/cfpt.php">Paper Submission Guidelines</a>) that briefly describes their system. If there are any questions, please let us know by raising an issue.
                                            </p>
                    <p class="card-text">
                        If there are any questions, please let us know by <a href="https://github.com/cmivqa/NLPCC-2023-Shared-Task-5/issues">raising an issue</a>.
                    </p>
                </div>
            </div>

            <div class="card cardx" id="dataset">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/dataset.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Dataset Overview</h5>
                          MMIVQA: Multilingual Medical Instructional Video Question Answering Challenge
                        </div>
                    </div>
                                        <div>
                    <center><img src="./img_cmivqa/pic4.png" width=80% style="margin-top: 15px"></center>
                                                                </div>
                    <div>
<center><h6><br>Fig. 4 Dataset examples of the MMIVQA shared task.</h6></center>
                    <p class="card-text">
                    The videos for this competition are crawled from the medical instructional channels on the YouTube website, where the subtitles (Both in Chinese and English) are obtained from the corresponding video. The question and corresponding temporal answer are manually labeled by annotators with the medical background. Each video may contain several questions-answer pairs, where the questions with the same semantic meanings correspond to a unique answer. The dataset is split into a training set, a validation set, and a test set. During the grand challenge, the test set along with the true “id” data number is not available to the public. The Fig. 4 shows the dataset examples for the mTAGV task. The “id” is the sample number that is used for the video retrieval track. The “video_id” means the unique ID from YouTube. The “Chinese_question” item is written manually by Chinese medical experts. The “English_question” is translated and corrected by native English-speaking doctors. The “start and end second” represents the temporal answer from the corresponding video. We also provide the video captions automatically generated from the video, including Chinese (Ch_caption) and English (Eng_caption) versions. As a result, our final goal is to retrieve the target video ID from the test corpus, and then locate the visual answer. More details about the dataset as well as the download links can be found in
                    <a href="https://github.com/cmivqa/NLPCC-2023-Shared-Task-5">https://github.com/cmivqa/NLPCC-2023-Shared-Task-5</a>.
                        </br>
                        </br>
                        Our baseline is released in <a href="https://github.com/Lireanstar/ACMMM2024_MMIVQA">https://github.com/Lireanstar/ACMMM2024_MMIVQA</a>. Any original methods (language/vision/audio/mutlimodal etc.) are welcome.
                   </p>
<!--                    <p>-->
<!--                        <a data-toggle="collapse" href="#collapseExample">-->
<!--                            <em>Show example videos from our ViCo dataset.</em>-->
<!--                        </a>-->
<!--                    </p>-->
<!--                    <div class="collapse" id="collapseExample">-->
<!--                        <div class="card cardx card-body">-->
<!--                            <div class="row" id="examples">-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/cte53x3i02lo.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/cte53x3i02lo.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/cz7zhbn7aq0u.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/cz7zhbn7aq0u.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/d336vte2a17j.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/d336vte2a17j.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/d5s6u96knvkp.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/d5s6u96knvkp.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/donfyjh9ge50.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/donfyjh9ge50.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/i0dqfrs6gz38.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/i0dqfrs6gz38.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                </div>-->
                <div class="card-body" id="downloads">
                    <div class="media">
                        <embed src="./img_cmivqa/download.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">Dataset Downloads</h5>
                        </div>
                    </div>
                    <p class="card-text">
<!--                        <strong>Train+Valid+Test Set:</strong> <a href="https://1drv.ms/u/s!Ag220j2nXkVswCSUXjTP_cA3Y0y2?e=9b3VXA" target="_blank">OneDrive</a><br/>-->
                        <strong>Train & Validation Set:</strong> <a href="https://pan.baidu.com/s/1eN_mo3iHyyHVH_8dciElqA?pwd=9871">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1QbY8DEaVLkY2w6vOCWAs4ZQFHgJ3q8ui?usp=sharing">GoogleDrive</a><br/>
                        <strong>Test Set A:</strong> <a href="https://pan.baidu.com/s/1XxoQ-KwUy9qf3Z0RfaPTaw?pwd=9874">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1Fdll0Qn8Ol65Z91u51NOEpBoxAQlrXN4?usp=sharing">GoogleDrive</a> <br/>
                        <strong>Test Set A Target:</strong> <a href="https://pan.baidu.com/s/1t52p6Wq-Fk5HRJGjUfDP_Q?pwd=2023">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1ibAKKlYMKXePEVA7p9ftRwiwSq0ytWOV?usp=sharing">GoogleDrive</a> <br/>
                    <strong>Test Set B:</strong> <a href="https://pan.baidu.com/s/1Z7wV76Yd2SDlVOUBkIafoQ?pwd=9871">BaiduNetDisk</a> | <a href="https://pan.baidu.com/s/1x3GRvKCMGcAFintmoSOuvw?pwd=9871">BaiduNetDisk_Backup</a> | <a href="https://drive.google.com/drive/folders/1QkddCTyeXYwO-gTtlC5r0e4M47Bdsclo?usp=sharing">GoogleDrive</a><br/>

                    <h5>Statistics</h5>
                    <center>
                   <table class="table table-striped table-bordered" style="text-align: center">
                        <tbody>
                              <tr>
                            <td>Dataset</td>
                            <td>Videos</td>
                            <td>QA pairs</td>
                            <td>Vocab Nums</td>
                            <td>Question Avg. Len.</td>
                            <td>Video Avg. Len.</td>
                        </tr>
                        <tr>
                            <td>Train &amp; Dev</td>
                            <td>1,228</td>
                            <td>2,937</td>
                            <td>3,125</td>
                            <td>17.16</td>
                            <td>263.3</td>
                        </tr>
                        <tr>
                            <td>Test A</td>
                            <td>200</td>
                            <td>492</td>
                            <td>2,171</td>
                            <td>17.81</td>
                            <td>242.4</td>
                        </tr>
                        <tr>
                            <td>Test B</td>
                            <td>200</td>
                            <td>511</td>
                            <td>2234</td>
                            <td>17.48</td>
                            <td>310.9</td>
                        </tr>
                        </tbody>
            </table>
                    </center>
                    <h5>Details</h5>
                    All the Train & Dev files include videos, audio, and the corresponding subtitles. The video and the corresponding audio come from Youtube Chinese medical channel,
                    which is obtained by using <a href="https://github.com/pytube">Pytube</a> tools. The subtitle are generated from the <a href="https://github.com/openai/whisper">Whisper</a>, which contains Simplified Chinese and Traditional Chinese tokens.
                    In order to unify the character types of questions and subtitles, we converted the above both into simplified Chinese. As for competition benchmarks, we recommend references [<a href="#ref1">1</a>-<a href="#ref2">2</a>], [<a href="#ref6">6</a>] as strong baselines. Beginners can quickly learn about the content of relevant competitions through references [<a href="#ref3">3</a>-<a href="#ref4">4</a>].
                    The Test A set and baseline are released, and any original methods (language/vision/audio/mutlimodal etc.) are welcome.

                        <!--                        <strong>Validation Set:</strong> <a> To be published around May 20th</a><br/>-->
<!--                        <em>Note: For Train Set, data in <code>listening_head.zip</code> contains the data in <code>talking_head.zip</code>.</em><br/>-->
<!--                        <u><strong><em>**Note:</em></strong></u> Except for the driver part, all other parts (e.g. render) can use additional data, but teams need to declare the pretrained model or additional data used when submission. For example, the use of a Talking Head Generation data is not allowed, but the use of pretrained data / model to adjust the render is allowed.-->
                    </p>
                    <h5>Guidelines</h5>
<!--                    <p class="card-text">-->
<!--                        <em>In Train Set</em>, for each track, the data consists of three parts:-->
<!--                        <ul>-->
<!--                            <li><code>videos/*.mp4</code>: all videos without audio track</li>-->
<!--                            <li><code>audios/*.wav</code>: all audios</li>-->
<!--                            <li>-->
<!--                                <code>*.csv</code>: return meta data about all videos/audios-->
<!--                                <table class="table table-hover table-sm">-->
<!--                                <thead>-->
<!--                                <tr><th scope="col">Name</th><th scope="col">Type</th><th scope="col">Description</th></tr>-->
<!--                                </thead>-->
<!--                                <tbody>-->
<!--                                    <tr><td>video_id</td><td>str</td><td>ID of video</td></tr>-->
<!--                                    <tr><td>uuid</td><td>str</td><td>ID of video sub-clips</td></tr>-->
<!--                                    <tr><td>speaker_id</td><td>int</td><td>ID of speaker</td></tr>-->
<!--                                    <tr><td>listener_id</td><td>int</td><td>ID of listener, <em>only in listening_head</em></td></tr>-->
<!--                                </tbody>-->
<!--                                </table>-->
<!--                                Given the uuid, the only audio <code>audios/{uuid}.wav</code> can be identified, and the listener&#39;s video is <code>videos/{uuid}.listener.mp4</code>, the speaker&#39;s video is <code>videos/{uuid}.speaker.mp4</code>.-->
<!--                            </li>-->
<!--                        </ul>-->
<!--                        <em>In Validation Set</em>, it organized as the final Test Set exclude the <code>output/</code> directory.<br/>The inputs consist of these parts:-->
<!--                        <ul>-->
<!--                            <li><code>videos/*.mp4</code>: speaker videos, <em>only in listening_head</em></li>-->
<!--                            <li><code>audios/*.wav</code>: all audios</li>-->
<!--                            <li><code>first_frames/*.jpg</code>: first frames of expected listener/speaker videos</li>-->
<!--                            <li><code>ref_images/(\d+).jpg</code>: reference images by person id</li>-->
<!--                            <li><code>*.csv</code>: return meta data about all videos/audios, same to CSVs in Train Set</li>-->
<!--                        </ul>-->
<!--                        Meanwhile, a baseline method and evaluation scripts is released in <a href="https://github.com/dc3ea9f/vico_challenge_baseline" target="_blank">github</a>.<br/>-->
<!--                        Example generations on train set:-->
<!--                        <div class="row" id="generations">-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/lz3g8clsk2z4.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/lz3g8clsk2z4.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/ek3utk9fn46v.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/ek3utk9fn46v.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/hk5ps1x9k2jo.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/hk5ps1x9k2jo.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                        </div>-->
<!--                    </p>-->
                    <div class="alert alert-dark" role="alert">
                        <h4 class="alert-heading">Terms and Conditions</h4>
<!--                        <p>The dataset users have requested permission to use the CMIVQA database. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>-->
<!--                        <p>The dataset users should submit a request (<a href="./CMIVQA_Dataset_Download_Agreement.docx" target="_blank">Dataset Agreement to <b>libincn@hnu.edu.cn</b></a>) for permission to use the CMIVQA dataset. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>-->
                        <p>The dataset users have requested permission to use the CMIVQA dataset. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>
                        <hr>
                        <ul>
                            <li>The dataset can only be used for non-commercial research and educational purposes.</li>
                            <li>The dataset must not be provided or shared in part or full with any third party.
                                </li>
                            <li>The researcher takes full responsibility for the usage of the dataset at any time, and the use of the YouTube videos must respect the YouTube Terms of Service.
                            </li>
                            <li>The authors of the dataset make no representations or warranties regarding the dataset, including but not limited to warranties of non-infringement or fitness for a particular purpose.
                            </li>
                            <li>You accept full responsibility for your use of the dataset and shall defend and indemnify the Authors of CMIVQA, against any and all claims arising from your use of the dataset, including but not limited to your use of any copies of copyrighted videos that you may create from the dataset.
                                </li>
                            <li>You may provide research associates and colleagues with access to the dataset provided that they first agree to be bound by these terms and conditions.
                            </li>
                            <!--li>
                                JD.co. reserve the right to terminate your access to the Database at any time.
                            </li-->
                            <li>
                               If you are employed by a for-profit, commercial entity, your employer shall also be bound by these terms and conditions, and you hereby represent that you are authorized to enter into this agreement on behalf of such employer.
                            </li>
                            <!--li>
                                The Intellectual Property Right Law of the People’s Republic of China apply to all
                                disputes under this agreement.
                            </li-->
                        </ul>
                    </div>
                </div>
            </div>

<!--            <div class="card cardx" id="submission">-->
<!--                <div class="card-body">-->
<!--                    <div class="media">-->
<!--                        <embed src="./img/star.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>-->
<!--                        <div class="media-body">-->
<!--                            <h5 class="mt-0">Competition Submission</h5>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                    <p class="card-text">-->
<!--                        Submission platform and Leaderboard are publicly available at this <a href="//vico.solutions">link</a>. Submission format and ranking rules are also included.-->
<!--                    </p>-->
<!--                </div>-->
                <div class="card-body" id="evaluation">
                    <div class="media">
                        <embed src="./img_cmivqa/balance.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                            <h5 class="mt-0-vico">Competition Evaluation</h5>
                        </div>
                    </div>
                    <p class="card-text">
                        The evaluation metrics of this challenge will be quantitative evaluated from the following perspectives:
                    <h5><b>Track 1</b></h5>
                    <ul>

                     <li><b>Multilingual Temporal Answer Grounding in Singe Video:</b><br/>
                         We will evaluate the results using the metric calculation equation shown as follows. Specifically, we use (1) Intersection over Union (IoU), and (2) mIoU which is the average IoU over all testing samples. Following the previous work [3]-[5], we adopt “R@n, IoU = μ”, and “mIoU” as the evaluation metrics, which treat localization of the frames in the video as a span prediction task. The “R@n, IoU = μ” denotes the Intersection over Union (IoU) of the predicted temporal answer span compared with the ground truth span, where the overlapping part is larger than “μ” in top-n retrieved moments. The “mIoU” is the average IoU over the samples. In our experiments, we use n = 1 and μ ∈ {0.3, 0.5, 0.7} to evaluate the mTAGSV results.                      $$
                    \begin{aligned}
                     \mathrm{IOU} & =\frac{A \cap B}{A \cup B}  \\
                    \mathrm{mIOU} & =\left(\sum_{i=1}^N \mathrm{IOU}\right) / N
                    \end{aligned}
                    $$
                    where A and B represent different spans, <i>N</i> = 3.<br/>
                    <b>Note:</b> The main ranking of this track is based on the mIoU score, and other metrics in this track are also provided for further analysis.</li>
                                            </ul>
                        <h5><b>Track 2</b></h5>
                                        <ul><li><b>Multilingual Video Corpus Retrieval:</b><br/>
                    Following the pioneering work [6], we adopt the video retrieval metric like “R@n”. Specifically, we adopt the n=1, 10, and 50 to denote the recall performance of the video retrieval. The Mean Reciprocal Rank (MRR) score to evaluate the Chinese medical instructional video corpus retrieval track, which can be calculated as follows.
                    $$
                    M R R=\frac{1}{|V|} \sum_{i=1}^{|V|} \frac{1}{\operatorname{Rank}_i}
                    $$
                        where the |<i>V</i>| is the number of the video corpus. For each testing sample <i>V</i><sub>i</sub>, the Rank<sub>i</sub> is the position of the target ground-truth video in the predicted list.<br/>
                    <b>Note:</b> The main ranking of this track is based on the Overall score. The Overall score is calculated by summarizing the R@1, R@10, R@50 and MRR scores, which is shown as follows.
                    $$
                    \text { Overall }=\sum_{i=1}^{|M|} {\text { Value}_i}
                    $$
                        where the |<i>M</i>| is the number of the evaluation metrics. Value<sub>i</sub> is the i-th metric in the above metrics (R@1, R@10, R@50 and MRR), |<i>M</i>|=4.
                    </li>                    </ul>
                    <h5><b>Track 3</b></h5>
                     <ul><li><b>Multilingual Temporal Answer Grounding in Video Corpus:</b><br/>
                         We kept the Intersection over Union (IoU) metric similar to the Track 1 and the retrieval indexes “R@n, n=1/10/50” and MRR similar to Track 2 for further analysis. The “R@n, IoU = 0.3/0.5/0.7” is still used, where we assign the n = 1, 10, 50 for evaluation. The index of mean IoU in video retrieval subtask, i.e., “R@1/10/50|mIOU”, is also adopted for measuring the average level of participating model’s performance.

                    <br/><b>Note:</b> The main ranking of this track is based on the Average score. The Average score is calculated by averaging the R@1|mIoU, R@10|mIoU, R@50|mIoU scores, which is shown as follows.
                    where A and B represent different spans.<br/>
                     $$
                    \text { Average }=\frac{1}{|M'|} \sum_{i=1}^{|M'|} \frac{1}{\text { Value}_i}
                    $$
                         where the |<i>M</i><sup>'</sup>| is the number of the evaluation metrics. Value<sub>i</sub> is the value of the i-th metric (i.e., R@1|mIoU, R@10|mIoU, R@50|mIoU), |<i>M</i><sup>'</sup>|=3.
<!--                            <li><strong>generation quality (image level)</strong>: SSIM, CPBD, PSNR</li>-->
<!--                            <li><strong>generation quality (feature level)</strong>: FID</li>-->
<!--                            <li><strong>identity preserving</strong>: Cosine Similarity (Arcface)</li>-->
<!--                            <li><strong>expression</strong>: L1 distance of 3dmm exp features</li>-->
<!--                            <li><strong>head motion</strong>: L1 distance of 3dmm angle &#38; trans features</li>-->
<!--                            <li><strong>lip sync (speaker only)</strong>: AV offset and AV confidence (SyncNet)</li>-->
<!--                            <li><strong>lip landmark distance</strong>: L1 distance of lip landmarks</li>-->
                        </ul>
                        Scripts can be accessed from <a href="https://github.com/cmivqa/NLPCC-2023-Shared-Task-5" target="_blank">this github repo</a>.
                        Baseline can be accessed from <a href="https://github.com/WENGSYX/CMIVQA_Baseline" target="_blank">this github repo</a>.
                    </p>
                </div>
            </div>

            <div class="card cardx" id="dates">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/calender.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">Important Dates</h5>
                          Important Dates and Details of the Chinese Medical Instructional Video Question Answering Challenge.
                        </div>
                      </div>
                    <p class="card-text">
                        <b>Signup to receive updates</b>: using <a href="http://tcci.ccf.org.cn/conference/2023/dldoc/NLPCC2023.SharedTask5.RegistrationForm.doc">this form</a>
                    </p>
                    <p>The submission deadline is at 11:59 p.m. of the stated deadline date (UTC/GMT+08:00).</p> </br>
                    <p>The results of test set A are updated every 7 days in Monday for 6 weeks (until May 21, 2023). Then the test set B will be released for the final evaluation. Participants can submit their results to the competition organizer via <a href="libincn@hnu.edu.cn">E-mail</a> for result registration, and each team does not exceed 3 submissions per day.</p>
                    <table class="table table-striped table-bordered">
                        <tbody>
                          <tr>
                            <td>Announcement of shared tasks and Registration open for participation</td>
                            <td>March 15, 2023</td>
                          </tr>
                          <tr>
                            <td>Release of detailed task guidelines & training data</td>
                            <td>April 3, 2023</td>
                          </tr>
                          <tr>
                            <td>Release of Test A data</td>
                            <td>April 10, 2023</td>
                          </tr>
                          <tr>
                            <td>Registration deadline (Same as the Test B submission deadline)</td>
                            <td>May 28, 2023</td>
                          </tr>
                          <tr>
                            <td>Release of test B data</td>
                            <td>May 21, 2023</td>
                          </tr>
                          <tr>
                            <td>Participants’ final result submission (Test B) deadline:</td>
                            <td>May 28, 2023</td>
                          </tr>
                          <tr>
                            <td>Evaluation results release and call for system reports and conference paper</td>
                            <td>June 10, 2023</td>
                          </tr>
                          <tr>
                            <td>Conference paper submission deadline (only for shared tasks)</td>
                            <td>June 30, 2023</td>
                          </tr>
                          <tr>
                            <td>Conference paper accept/reject notification</td>
                            <td>July 18, 2023</td>
                          </tr>
                          <tr>
                            <td>Camera-ready paper submission deadline</td>
                            <td>August 1, 2023</td>
                          </tr>

                        </tbody>
                    </table>
                </div>
            </div>


            <div class="card cardx" id="award">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/award.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">Award</h5>

                          Awards will be granted to the top 3 teams. Every track has three prizes (￥3000, ￥2000, ￥1000), where all prizes count ￥18000.
                        </div>
                        </div>
                          The top 1 participating team in each track will be certificated by NLPCC and CCF-NLP. Cash rewards are granted for the top-3 teams for each track.
                            </br>
                            </br>
                            The first prize (*1): ￥3000
                            </br>
                            The second prize (*1): ￥2000
                            </br>
                            The third  prize (*1): ￥1000
                    </br>
                    </br>
                    The final interpretation right of the award belongs to Hunan University.
                </div>
            </div>


            <div class="card cardx" id="organizers">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/orgnizer.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Organizers</h5>
                          Organizers of this Chinese Medical Instructional Video Question Answering Challenge
                        </div>
                      </div>
                    <div class="row">
                    <div class="col orgnizer-box">
                            <img src="./img_cmivqa/li.jpg" class="rounded-circle orgnizer-img" width=100/>
                            <p class="orgnizer-text">Shutao Li<br>College of Electrical and Information Engineering, Hunan University</p>
                    </div>
                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/sunbin.jpg" class="rounded-circle orgnizer-img" width=100/>
                            <p class="orgnizer-text">Bin Sun<br>College of Electrical and Information Engineering, Hunan University</p>
                        </div>

                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/libin.png" class="rounded-circle orgnizer-img" width=100/>
                            <p class="orgnizer-text">Bin Li<br>College of Electrical and Information Engineering, Hunan University</p>
                        </div>
                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/yixuan.jpg" class="rounded-circle orgnizer-img" width=95/>
                            <p class="orgnizer-text">Yixuan Weng<br>National Laboratory of Pattern Recognition Institute of Automation, CAS</p>
                        </div>
                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/guohu.jpg" class="rounded-circle orgnizer-img" width=100/>
                            <p class="orgnizer-text">Hu Guo<br>College of Electrical and Information Engineering, Hunan University</p>
                        </div>
                </div>
            </div>

            <div class="card cardx" id="reference">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/icon-paper.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">References</h5>
                          Some related works of the Chinese Medical Instructional Video Question Answering Challenge.
                        </div>
                      </div>
                    <div id=ref1>[1] <i>Li, Bin, et al. “Towards visual-prompt temporal answering grounding in medical instructional video.” arXiv preprint arXiv:2203.06667 (2022). </i></div>
                    <div id=ref2>[2] <i>Weng, Yixuan, and Bin Li. “Visual Answer Localization with Cross-modal Mutual Knowledge Transfer.” ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095026.</i></div>
                    <div id=ref3>[3] <i>Deepak Gupta, Kush Attal, and Dina Demner-Fushman. “A Dataset for Medical Instructional Video Classification and Question Answering.” arXiv preprint arXiv:2201.12888, 2022. </i></div>
                    <div id=ref4>[4] <i>Deepak Gupta, and Dina Demner-Fushman. “Overview of the MedVidQA 2022 Shared Task on Medical Video Question-Answering. ” BioNLP 2022@ ACL 2022 (2022): 264.</i></div>
                    <div id=ref5>[5] <i>Zhang, Hao, et al. “Natural language video localization: A revisit in span-based question answering framework.” IEEE transactions on pattern analysis and machine intelligence 44.8 (2021): 4252-4266.</i></div>
                    <div id=ref6>[6] <i>Li, Bin, et al. "Learning to Locate Visual Answer in Video Corpus Using Question." ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10096391.</i></div>




                </div>
            </div>
        </section>

    </main>

    <footer class="footer">
        <p class="footer-text">College of Electrical and Information Engineering, Hunan University</p>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous">
    </script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous">
    </script>
</body>

</html>
