<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
        integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/svg+xml" href="./img_cmivqa/logo2.svg" sizes="any">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
<!-- 插入公式 -->
</script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BKN0PM1J6W"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-BKN0PM1J6W');
    </script>
    <title>Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="nav">
        <div class="container"><a href="/" class="navbar-brand">M4IVQA</a>
            <ul class="navbar-nav mr-auto order-1">
                <li class="nav-item"><a class="nav-link" href="#challenge">Overview</a></li>
                <li class="nav-item"><a class="nav-link" href="#dataset">Dataset</a></li>
                <li class="nav-item"><a class="nav-link" href="#dates">Dates</a></li>
                <li class="nav-item"><a class="nav-link" href="#award">Award</a></li>
                <li class="nav-item"><a class="nav-link" href="#organizers">Organizers</a></li>
                <li class="nav-item"><a class="nav-link" href="#reference">Reference</a></li>
                <li class="nav-item"><a class="nav-link" href="./leaderboard.html">Leaderboard2024</a></li>
                <li class="nav-item"><a class="nav-link" href="./leaderboard2.html">Leaderboard2025</a></li>
                <!-- <li class="nav-item"><a class="nav-link" href="./leaderboardb.html">LeaderboardB</a></li> -->
<!--                TODO leaderboard-->
            </ul>
            <ul class="navbar-nav ml-auto order-1">
<!--                <li class="nav-item"><a class="nav-link" href="https://project.mhzhou.com/vico/">Full Dataset</a></li>-->
<!--                <li class="nav-item"><a class="nav-link" href="https://arxiv.org/abs/2112.13548">Paper</a></li>-->
                <li class="nav-item"><a class="nav-link" href="https://github.com/Lireanstar/NLPCC2024_MMIVQA">Code</a></li>
                <li class="nav-item"><a class="nav-link" href="https://github.com/WENGSYX/MMIVQA_Baseline">Baseline</a></li>
            </ul>
        </div>
        </div>
    </nav>

    <main id="main" class="site-main main">
        <section class="container" id="banner">
            
            <div class="title-text-box">

                
                <h3 class="display-5 text-bold">Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</h3>
                <h5 class="text-regular">held in NLPCC 2025</h5>
                                <img src="./img_cmivqa/NLPCC_2025.jpg" width=100%>
            </div>
            
            <br>
            <a class="btn" id="learn-more-btn" href="#challenge" role="button">Learn more</a>
            <a class="btn" id="download-btn" href="#downloads" role="button">Download dataset</a>
        </section>

        <section class="container">
            <div class="card cardx" id="challenge">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/prize.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Challenge Overview</h5>
                          Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge
                        </div>
                      </div>
                    <p class="card-text">
                        Designing models that can comprehend multi-modal (text, speech, and image/video) instructional video in the medical domain, process multilingual data, and locate multi-hop questions in the video is an emerging challenge. Following the successful hosts of the 1-st (NLPCC 2023 Foshan) and the 2-rd (NLPCC 2024 Hangzhou) CMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop (m^3) question answering systems, with a specific focus on medical instructional videos. This task focuses on evaluating models that can integrate information from medical instructional videos, understand multiple languages, and answer complex, multi-hop questions that require reasoning over various modalities.  Participants are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. Models will be evaluated on the relevance of their answers, as well as their ability to handle complex multi-modal and multilingual inputs. 
                    </p>
                    <p class="card-text">The task consists of multiple stages, including training, testing, and evaluation, which contains three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Singe Video (m^3TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (m^3VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (m^3TAGVC).</p>

                    <center><img src="./img_cmivqa/track1.png" width=90%></center>
                    <div>
<center><h6><br>Fig. 1: Illustration of Multilingual Temporal Answer Grounding in Singe Video (mTAGSV). </h6></center>
                        <ul>
</div>
                        <li ><b>Track 1. Multilingual Temporal Answer Grounding in Singe Video (mTAGSV):</b> As shown in Fig. 1: given a medical or health-related question and a single untrimmed Chinese medical instructional video, this track aims to locate the temporal answer (start and end time points) within the video.</li><!--li>Given the identity and audio signals of the speaker, participants are challenged to develop an algorithm to generate a video that can match the audio signals.</li-->
                </ul>
                    <center><img src="./img_cmivqa/track2.png" width=90% style="margin-top: 15px"></center>
                    <div>
<center><h6><br>Fig. 2: Multilingual Illustration of Video Corpus Retrieval (mVCR). </h6></center>
</div>
                    <ul>
                    <li><b>Track 2. Multilingual Video Corpus Retrieval (mVCR):</b> As shown in Fig. 2, given a medical or health-related question and a large collection of untrimmed bilingual medical instructional videos, this track aims to find the most relevant video corresponding to the given question in the video corpus.</li><!--li>Given the identity of the listener, and a corresponding talking head video & audio, participants are challenged to develop an algorithm to generate a video that can respond to the speaker's behaviors actively in real-time.</li-->
                   </ul>
                        <center> <img src="./img_cmivqa/track3.png" width=90% style="margin-top: 15px"></center>
                    <div>
<center><h6><br>Fig. 3: Illustration of Multilingual Temporal Answer Grounding in Video Corpus (mTAGVC).</h6></center>
<ul>
                    <li><b>Track 3. Multilingual Temporal Answer Grounding in Video Corpus (mTAGVC):</b> As shown in Fig. 3, given a text question and a large collection of untrimmed Chinese medical instructional videos, this track aims at finding the matching video answer span within the most relevant video corresponding to the given question in the video corpus.</li><!--li>Given the identity and audio signals of the speaker, participants are challenged to develop an algorithm to generate a video that can match the audio signals.</li-->
                </ul>
                    </p>
                    <!--
                    <b>Vivid talking head video generation</b> conditioned on the identity and audio signals of the speaker; <b>Responsive listening head video generation</b> conditioned on the identity of the listener and with responding to the speaker’s behaviors in real-time. The main difficulties lie in the large amount of products labels, and fine-grained details of similar products. Top-1 and Top-5 accuracy is measured to rank each submission.
                    </p>
                    -->
                    <p class="card-text">
                        The team constitution (members of a team) cannot be changed after the evaluation period has begun. Individuals and teams with top submissions will present their work at the workshop. We also encourage every team to upload a paper that briefly describes their system. If there are any questions, please let us know by raising an issue.
                                            </p>
                    <p class="card-text">
                        If there are any questions, please let us know by <a href="https://github.com/Lireanstar/NLPCC2024_MMIVQA/issues">raising an issue</a>.
                    </p>
                </div>
            </div>

            <div class="card cardx" id="dataset">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/dataset.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Dataset Overview</h5>
                          MMIVQA: Multilingual Medical Instructional Video Question Answering Challenge
                        </div>
                    </div>
                                        <div>
                    <center><img src="./img_cmivqa/pic4.png" width=80% style="margin-top: 15px"></center>
                                                                </div>
                    <div>
<center><h6><br>Fig. 4 Dataset examples of the MMIVQA shared task.</h6></center>
                    <p class="card-text">
                    The videos for this competition are crawled from the medical instructional channels on the YouTube website, where the subtitles (Both in Chinese and English) are obtained from the corresponding video. The question and corresponding temporal answer are manually labeled by annotators with the medical background. Each video may contain several questions-answer pairs, where the questions with the same semantic meanings correspond to a unique answer. The dataset is split into a training set, a validation set, and a test set. During the grand challenge, the test set along with the true “id” data number is not available to the public. The Fig. 4 shows the dataset examples for the mTAGV task. The “id” is the sample number that is used for the video retrieval track. The “video_id” means the unique ID from YouTube. The “Chinese_question” item is written manually by Chinese medical experts. The “English_question” is translated and corrected by native English-speaking doctors. The “start and end second” represents the temporal answer from the corresponding video. We also provide the video captions automatically generated from the video, including Chinese (Ch_caption) and English (Eng_caption) versions. As a result, our final goal is to retrieve the target video ID from the test corpus, and then locate the visual answer. More details about the dataset as well as the download links can be found in
                    <a href="https://github.com/WENGSYX/MMIVQA_Baseline">https://github.com/Lireanstar/NLPCC2024_MMIVQA</a>.
                        </br>
                        </br>
                        Our baseline method will be released in <a href="https://github.com/WENGSYX/MMIVQA_Baseline">https://github.com/WENGSYX/CMIVQA_Baseline</a>. 
                        </br>
                        Any original methods (language/vision/audio/multimodal etc.) are welcome.
                   </p>
<!--                    <p>-->
<!--                        <a data-toggle="collapse" href="#collapseExample">-->
<!--                            <em>Show example videos from our ViCo dataset.</em>-->
<!--                        </a>-->
<!--                    </p>-->
<!--                    <div class="collapse" id="collapseExample">-->
<!--                        <div class="card cardx card-body">-->
<!--                            <div class="row" id="examples">-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/cte53x3i02lo.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/cte53x3i02lo.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/cz7zhbn7aq0u.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/cz7zhbn7aq0u.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/d336vte2a17j.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/d336vte2a17j.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/d5s6u96knvkp.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/d5s6u96knvkp.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/donfyjh9ge50.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/donfyjh9ge50.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                                <div class="col-md-4">-->
<!--                                    <div class="embed-responsive embed-responsive-16by9">-->
<!--                                        <video controls="true" class="embed-responsive-item">-->
<!--                                            <source src="media/examples/i0dqfrs6gz38.mp4" type="video/mp4" />-->
<!--                                            <p>Your browser doesn't support HTML5 video. Here is a <a href="media/examples/i0dqfrs6gz38.mp4">link to the-->
<!--                                                    video</a> instead.</p>-->
<!--                                        </video>-->
<!--                                    </div>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                </div>-->
                <div class="card-body" id="downloads">
                    <div class="media">
                        <embed src="./img_cmivqa/download.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">Dataset Downloads</h5>
                        </div>
                    </div>
                    <p class="card-text">
<!--                        <strong>Train+Valid+Test Set:</strong> <a href="https://1drv.ms/u/s!Ag220j2nXkVswCSUXjTP_cA3Y0y2?e=9b3VXA" target="_blank">OneDrive</a><br/>-->
                        <strong>Train Set:</strong> <a href="https://pan.baidu.com/s/1eN_mo3iHyyHVH_8dciElqA?pwd=9871">BaiduNetDisk</a> <br/>
                        <strong>Dev Set:</strong> <a href="https://pan.baidu.com/s/1G1u-nnOm50_d3cl-vcqimQ?pwd=9871">BaiduNetDisk</a> <br/>
                         <strong>Test Set:</strong> <a href="https://pan.baidu.com/s/15WCJmm3MgB6b8HFKIki8ag?pwd=9871">BaiduNetDisk</a> <br/>                       
                        <!-- <strong>Train & Validation Set:</strong> <a href="https://pan.baidu.com/s/1eN_mo3iHyyHVH_8dciElqA?pwd=9871">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1QbY8DEaVLkY2w6vOCWAs4ZQFHgJ3q8ui?usp=sharing">GoogleDrive</a><br/> -->

<!--                         <strong>Test Set A:</strong> <a href="https://pan.baidu.com/s/1XxoQ-KwUy9qf3Z0RfaPTaw?pwd=9874">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1Fdll0Qn8Ol65Z91u51NOEpBoxAQlrXN4?usp=sharing">GoogleDrive</a> <br/> -->
<!--                         <strong>Test Set A Target:</strong> <a href="https://pan.baidu.com/s/1t52p6Wq-Fk5HRJGjUfDP_Q?pwd=2023">BaiduNetDisk</a> | <a href="https://drive.google.com/drive/folders/1ibAKKlYMKXePEVA7p9ftRwiwSq0ytWOV?usp=sharing">GoogleDrive</a> <br/> -->
<!--                     <strong>Test Set B:</strong> <a href="https://pan.baidu.com/s/1Z7wV76Yd2SDlVOUBkIafoQ?pwd=9871">BaiduNetDisk</a> | <a href="https://pan.baidu.com/s/1x3GRvKCMGcAFintmoSOuvw?pwd=9871">BaiduNetDisk_Backup</a> | <a href="https://drive.google.com/drive/folders/1QkddCTyeXYwO-gTtlC5r0e4M47Bdsclo?usp=sharing">GoogleDrive</a><br/> -->
                    <h5>Statistics</h5>
                    <center>
                   <table class="table table-striped table-bordered" style="text-align: center">
                        <tbody>
                              <tr>
                            <td>Dataset</td>
                            <td>Videos</td>
                            <td>QA pairs</td>
                            <td>Vocab Nums</td>
                            <td>Ch_Question Avg. Len.</td>
                              <td>Eng_Question Avg. Len.</td>
                            <td>Video Avg. Len.</td>
                        </tr>
                        <!-- 
                            <td>Train &amp; Dev</td>
                            <td>1,228</td>
                            <td>5840</td>
                            <td>6582</td>
                            <td>17.16</td>
                            <td>6.97</td>
                            <td>263.3</td>
                        </tr -->
                            <td>Train Set</td>
                            <td>1228</td>
                            <td>5840</td>
                            <td>6582</td>
                            <td>17.16</td>
                            <td>6.97</td>
                            <td>263.3</td>
                        </tr>
                         <tr>
                            <td>Dev Set</td>
                            <td>200</td>
                            <td>983</td>
                            <td>1743</td>
                             <td>17.81</td>
                            <td>7.26</td>
                            <td>242.4</td>
                        </tr>

<!--                         <tr>
                            <td>Test A</td>
                            <td>200</td>
                            <td>492</td>
                            <td>2,171</td>
                            <td>17.81</td>
                            <td>242.4</td>
                        </tr> -->
                        <tr>
                            <td>Test Set</td>
                            <td>200</td>
                            <td>1022</td>
                            <td>2234</td>
                            <td>18.22</td>     
                            <td>7.44</td>
                            <td>310.9</td>
                        </tr>
                        </tbody>
            </table>
                    </center>
                    <h5>Details</h5>
                    All the Train & Dev files include videos, audio, and the corresponding subtitles. The video and the corresponding audio come from Youtube Chinese medical channel,
                    which is obtained by using <a href="https://github.com/pytube">Pytube</a> tools. The subtitle are generated from the <a href="https://github.com/openai/whisper">Whisper</a>, which contains Simplified Chinese and Traditional Chinese tokens.
                    In order to unify the character types of questions and subtitles, we converted the above both into simplified Chinese. As for competition benchmarks, we recommend references [<a href="#ref1">1</a>-<a href="#ref2">2</a>], [<a href="#ref6">6</a>] as strong baselines. Beginners can quickly learn about the content of relevant competitions through references [<a href="#ref3">3</a>-<a href="#ref4">4</a>].
                    The Test A set and baseline are released, and any original methods (language/vision/audio/multimodal etc.) are welcome.

                        <!--                        <strong>Validation Set:</strong> <a> To be published around May 20th</a><br/>-->
<!--                        <em>Note: For Train Set, data in <code>listening_head.zip</code> contains the data in <code>talking_head.zip</code>.</em><br/>-->
<!--                        <u><strong><em>**Note:</em></strong></u> Except for the driver part, all other parts (e.g. render) can use additional data, but teams need to declare the pretrained model or additional data used when submission. For example, the use of human labeled data is not allowed, but the use of pretrained data / model to be finetuned is allowed.-->
                    </p>
                    <h5>Guidelines</h5>
<!--                    <p class="card-text">-->
<!--                        <em>In Train Set</em>, for each track, the data consists of three parts:-->
<!--                        <ul>-->
<!--                            <li><code>videos/*.mp4</code>: all videos without audio track</li>-->
<!--                            <li><code>audios/*.wav</code>: all audios</li>-->
<!--                            <li>-->
<!--                                <code>*.csv</code>: return meta data about all videos/audios-->
<!--                                <table class="table table-hover table-sm">-->
<!--                                <thead>-->
<!--                                <tr><th scope="col">Name</th><th scope="col">Type</th><th scope="col">Description</th></tr>-->
<!--                                </thead>-->
<!--                                <tbody>-->
<!--                                    <tr><td>video_id</td><td>str</td><td>ID of video</td></tr>-->
<!--                                    <tr><td>uuid</td><td>str</td><td>ID of video sub-clips</td></tr>-->
<!--                                    <tr><td>speaker_id</td><td>int</td><td>ID of speaker</td></tr>-->
<!--                                    <tr><td>listener_id</td><td>int</td><td>ID of listener, <em>only in listening_head</em></td></tr>-->
<!--                                </tbody>-->
<!--                                </table>-->
<!--                                Given the uuid, the only audio <code>audios/{uuid}.wav</code> can be identified, and the listener&#39;s video is <code>videos/{uuid}.listener.mp4</code>, the speaker&#39;s video is <code>videos/{uuid}.speaker.mp4</code>.-->
<!--                            </li>-->
<!--                        </ul>-->
<!--                        <em>In Validation Set</em>, it organized as the final Test Set exclude the <code>output/</code> directory.<br/>The inputs consist of these parts:-->
<!--                        <ul>-->
<!--                            <li><code>videos/*.mp4</code>: speaker videos, <em>only in listening_head</em></li>-->
<!--                            <li><code>audios/*.wav</code>: all audios</li>-->
<!--                            <li><code>first_frames/*.jpg</code>: first frames of expected listener/speaker videos</li>-->
<!--                            <li><code>ref_images/(\d+).jpg</code>: reference images by person id</li>-->
<!--                            <li><code>*.csv</code>: return meta data about all videos/audios, same to CSVs in Train Set</li>-->
<!--                        </ul>-->
<!--                        Meanwhile, a baseline method and evaluation scripts is released in <a href="https://github.com/dc3ea9f/vico_challenge_baseline" target="_blank">github</a>.<br/>-->
<!--                        Example generations on train set:-->
<!--                        <div class="row" id="generations">-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/lz3g8clsk2z4.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/lz3g8clsk2z4.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/ek3utk9fn46v.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/ek3utk9fn46v.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                            <div class="col">-->
<!--                                <div class="embed-responsive embed-responsive-16by9">-->
<!--                                    <video controls="true" class="embed-responsive-item">-->
<!--                                        <source src="media/hk5ps1x9k2jo.mp4" type="video/mp4" />-->
<!--                                        <p>Your browser doesn't support HTML5 video. Here is a <a href="media/hk5ps1x9k2jo.mp4">link to the video</a> instead.</p>-->
<!--                                    </video>-->
<!--                                </div>-->
<!--                            </div>-->
<!--                        </div>-->
<!--                    </p>-->
                    <div class="alert alert-dark" role="alert">
                        <h4 class="alert-heading">Terms and Conditions</h4>
<!--                        <p>The dataset users have requested permission to use the MMIVQA database. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>-->
<!--                        <p>The dataset users should submit a request (<a href="./CMIVQA_Dataset_Download_Agreement.docx" target="_blank">Dataset Agreement to <b>libincn@hnu.edu.cn</b></a>) for permission to use the MMIVQA dataset. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>-->
                        <p>The dataset users have requested permission to use the MMIVQA dataset. In exchange for such permission, the users hereby agree to the following terms and conditions:</p>
                        <hr>
                        <ul>
                            <li>The dataset can only be used for non-commercial research and educational purposes.</li>
                            <li>The dataset must not be provided or shared in part or full with any third party.
                                </li>
                            <li>The researcher takes full responsibility for the usage of the dataset at any time, and the use of the YouTube videos must respect the YouTube Terms of Service.
                            </li>
                            <li>The authors of the dataset make no representations or warranties regarding the dataset, including but not limited to warranties of non-infringement or fitness for a particular purpose.
                            </li>
                            <li>You accept full responsibility for your use of the dataset and shall defend and indemnify the Authors of MMIVQA, against any and all claims arising from your use of the dataset, including but not limited to your use of any copies of copyrighted videos that you may create from the dataset.
                                </li>
                            <li>You may provide research associates and colleagues with access to the dataset provided that they first agree to be bound by these terms and conditions.
                            </li>
                            <!--li>
                                JD.co. reserve the right to terminate your access to the Database at any time.
                            </li-->
                            <li>
                               If you are employed by a for-profit, commercial entity, your employer shall also be bound by these terms and conditions, and you hereby represent that you are authorized to enter into this agreement on behalf of such employer.
                            </li>
                            <!--li>
                                The Intellectual Property Right Law of the People’s Republic of China apply to all
                                disputes under this agreement.
                            </li-->
                        </ul>
                    </div>
                </div>
            </div>

<!--            <div class="card cardx" id="submission">-->
<!--                <div class="card-body">-->
<!--                    <div class="media">-->
<!--                        <embed src="./img/star.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>-->
<!--                        <div class="media-body">-->
<!--                            <h5 class="mt-0">Competition Submission</h5>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                    <p class="card-text">-->
<!--                        Submission platform and Leaderboard are publicly available at this <a href="//vico.solutions">link</a>. Submission format and ranking rules are also included.-->
<!--                    </p>-->
<!--                </div>-->
                <div class="card-body" id="evaluation">
                    <div class="media">
                        <embed src="./img_cmivqa/balance.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                            <h5 class="mt-0-vico">Competition Evaluation</h5>
                        </div>
                    </div>
                    <p class="card-text">
                        The evaluation metrics of this challenge will be quantitative evaluated from the following perspectives:
                    <h5><b>Track 1</b></h5>
                    <ul>

                     <li><b>Multilingual Temporal Answer Grounding in Singe Video:</b><br/>
                         We will evaluate the results using the metric calculation equation shown as follows. Specifically, we use (1) Intersection over Union (IoU), and (2) mIoU which is the average IoU over all testing samples. Following the previous work [3]-[5], we adopt “R@n, IoU = μ”, and “mIoU” as the evaluation metrics, which treat localization of the frames in the video as a span prediction task. The “R@n, IoU = μ” denotes the Intersection over Union (IoU) of the predicted temporal answer span compared with the ground truth span, where the overlapping part is larger than “μ” in top-n retrieved moments. The “mIoU” is the average IoU over the samples. In our experiments, we use n = 1 and μ ∈ {0.3, 0.5, 0.7} to evaluate the TAGSV results.                      $$
                    \begin{aligned}
                     \mathrm{IOU} & =\frac{A \cap B}{A \cup B}  \\
                    \mathrm{mIOU} & =\left(\sum_{i=1}^N \mathrm{IOU}\right) / N
                    \end{aligned}
                    $$
                    where A and B represent different spans, <i>N</i> = 3.<br/>
                    <b>Note:</b> The main ranking of this track is based on the mIoU score, and other metrics in this track are also provided for further analysis.</li>
                                            </ul>
                        <h5><b>Track 2</b></h5>
                                        <ul><li><b>Multilingual Video Corpus Retrieval </b><br/>
                    Following the pioneering work [6], we adopt the video retrieval metric like “R@n”. Specifically, we adopt the n=1, 10, and 50 to denote the recall performance of the video retrieval. The Mean Reciprocal Rank (MRR) score to evaluate the Chinese medical instructional video corpus retrieval track, which can be calculated as follows.
                    $$
                    M R R=\frac{1}{|V|} \sum_{i=1}^{|V|} \frac{1}{\operatorname{Rank}_i}
                    $$
                        where the |<i>V</i>| is the number of the video corpus. For each testing sample <i>V</i><sub>i</sub>, the Rank<sub>i</sub> is the position of the target ground-truth video in the predicted list.<br/>
                    <b>Note:</b> The main ranking of this track is based on the Overall score. The Overall score is calculated by summarizing the R@1, R@10, R@50 and MRR scores, which is shown as follows.
                    $$
                    \text { Overall }=\sum_{i=1}^{|M|} {\text { Value}_i}
                    $$
                        where the |<i>M</i>| is the number of the evaluation metrics. Value<sub>i</sub> is the i-th metric in the above metrics (R@1, R@10, R@50 and MRR), |<i>M</i>|=4.
                    </li>                    </ul>
                    <h5><b>Track 3</b></h5>
                     <ul><li><b>Multilingual Temporal Answer Grounding in Video Corpus:</b><br/>
                         We kept the Intersection over Union (IoU) metric similar to the Track 1 and the retrieval indexes “R@n, n=1/10/50” and MRR similar to Track 2 for further analysis. The “R@n, IoU = 0.3/0.5/0.7” is still used, where we assign the n = 1, 10, 50 for evaluation. The index of mean IoU in video retrieval subtask, i.e., “R@1/10/50|mIOU”, is also adopted for measuring the average level of participating model’s performance.

                    <br/><b>Note:</b> The main ranking of this track is based on the Average score. The Average score is calculated by averaging the R@1|mIoU, R@10|mIoU, R@50|mIoU scores, which is shown as follows.
                    where A and B represent different spans.<br/>
                     $$
                    \text { Average }=\frac{1}{|M'|} \sum_{i=1}^{|M'|} \frac{1}{\text { Value}_i}
                    $$
                         where the |<i>M</i><sup>'</sup>| is the number of the evaluation metrics. Value<sub>i</sub> is the value of the i-th metric (i.e., R@1|mIoU, R@10|mIoU, R@50|mIoU), |<i>M</i><sup>'</sup>|=3.
<!--                            <li><strong>generation quality (image level)</strong>: SSIM, CPBD, PSNR</li>-->
<!--                            <li><strong>generation quality (feature level)</strong>: FID</li>-->
<!--                            <li><strong>identity preserving</strong>: Cosine Similarity (Arcface)</li>-->
<!--                            <li><strong>expression</strong>: L1 distance of 3dmm exp features</li>-->
<!--                            <li><strong>head motion</strong>: L1 distance of 3dmm angle &#38; trans features</li>-->
<!--                            <li><strong>lip sync (speaker only)</strong>: AV offset and AV confidence (SyncNet)</li>-->
<!--                            <li><strong>lip landmark distance</strong>: L1 distance of lip landmarks</li>-->
                        </ul>
                        Scripts can be accessed from <a href="https://github.com/Lireanstar/NLPCC2024_MMIVQA" target="_blank">this github repo</a>.
                        Baseline can be accessed from <a href="https://github.com/WENGSYX/MMIVQA_Baseline" target="_blank">this github repo</a>.
                    </p>
                </div>
            </div>

            <div class="card cardx" id="dates">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/calender.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">Important Dates</h5>
                          Important Dates and Details of the Chinese Medical Instructional Video Question Answering Challenge.
                        </div>
                      </div>
                    <p class="card-text">
                        <b>Signup to receive updates</b>: using <a href="http://tcci.ccf.org.cn/conference/2024/dldoc/NLPCC2024.SharedTask7.RegistrationForm.doc">this form</a>
                    </p>
                    <p>The submission deadline is at 11:59 p.m. of the stated deadline date (UTC/GMT+08:00).</p> </br>
                    <p>During the training and verification phases, we released the complete data set (including corresponding labels) for the participating teams to freely choose their models. During the test set release phase, we will release additional test sets and will maintain the frequency of updating the online list every day. Participants can submit their results to the competition organizer via <a href="libincn@hnu.edu.cn">E-mail</a> for result registration, and each team does not exceed 5 submissions in total.</p>
                    <table class="table table-striped table-bordered">
                        <tbody>
                          <tr>
                            <td>Announcement of shared tasks and Registration open for participation</td>
                            <td>March 25, 2024</td>
                          </tr>
                          <tr>
                            <td>Release of detailed task guidelines & training data</td>
                            <td>April 15, 2024</td>
                          </tr>
                          <tr>
                            <td>Release of test data</td>
                            <td>June 11, 2024</td>
                          </tr>
                          <tr>
                            <td>Test submission deadline</td>
                            <td>June 20, 2024</td>
                          </tr>
                          <tr>
                            <td>Registration deadline (Same as the Test submission deadline)</td>
                            <td>June 20, 2024</td>
                          </tr>
                          <tr>
                            <td>Evaluation results release and call for system reports and conference paper</td>
                            <td>June 30, 2024</td>
                          </tr>
                        </tbody>
                    </table>
                </div>
            </div>



            <div class="card cardx" id="organizers">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/orgnizer.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0">Organizers</h5>
                          Organizers of this Chinese Medical Instructional Video Question Answering Challenge
                        </div>
                      </div>
                    <div class="row">
                    <div class="col orgnizer-box">
                            <img src="./img_cmivqa/zhou.jpg" class="rounded-circle orgnizer-img" width=108/>
                            <p class="orgnizer-text">Shoujun Zhou<br>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</p>
                    </div>

                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/libin.png" class="rounded-circle orgnizer-img" width=109/>
                            <p class="orgnizer-text">Bin Li<br>College of Electrical and Information Engineering, Hunan University</p>
                        </div>
                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/song.jpg" class="rounded-circle orgnizer-img" width=105/>
                            <p class="orgnizer-text">Qiya Song<br>Hunan Normal University</p>
                        </div>
<!--                         <div class="col orgnizer-box">
                            <img src="./img_cmivqa/sunbin.jpg" class="rounded-circle orgnizer-img" width=100/>
                            <p class="orgnizer-text">Bin Sun<br>College of Electrical and Information Engineering, Hunan University</p>
                        </div> -->
                        <div class="col orgnizer-box">
                            <img src="./img_cmivqa/min.jpg" class="rounded-circle orgnizer-img" width=95/>
                            <p class="orgnizer-text">Xianwen Min<br>School of Robotics, Hunan University</p>
                        </div>
                </div>
            </div>

            <div class="card cardx" id="reference">
                <div class="card-body">
                    <div class="media">
                        <embed src="./img_cmivqa/icon-paper.svg" type="image/svg+xml" class="mr-3 align-self-center" width=40/>
                        <div class="media-body">
                          <h5 class="mt-0-vico">References</h5>
                          Some related works of the Chinese Medical Instructional Video Question Answering Challenge.
                        </div>
                      </div>
                    <div id=ref1>[1] <i>Li, Bin, et al. “Towards visual-prompt temporal answering grounding in medical instructional video.” arXiv preprint arXiv:2203.06667 (2022). </i></div>
                    <div id=ref2>[2] <i>Weng, Yixuan, and Bin Li. “Visual Answer Localization with Cross-modal Mutual Knowledge Transfer.” ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095026.</i></div>
                    <div id=ref3>[3] <i>Deepak Gupta, Kush Attal, and Dina Demner-Fushman. “A Dataset for Medical Instructional Video Classification and Question Answering.” arXiv preprint arXiv:2201.12888, 2022. </i></div>
                    <div id=ref4>[4] <i>Deepak Gupta, and Dina Demner-Fushman. “Overview of the MedVidQA 2022 Shared Task on Medical Video Question-Answering. ” BioNLP 2022@ ACL 2022 (2022): 264.</i></div>
                    <div id=ref5>[5] <i>Zhang, Hao, et al. “Natural language video localization: A revisit in span-based question answering framework.” IEEE transactions on pattern analysis and machine intelligence 44.8 (2021): 4252-4266.</i></div>
                    <div id=ref6>[6] <i>Li, Bin, et al. "Learning to Locate Visual Answer in Video Corpus Using Question." ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10096391.</i></div>




                </div>
            </div>
        </section>

    </main>

    <footer class="footer">
        <p class="footer-text">College of Electrical and Information Engineering, Hunan University</p>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous">
    </script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous">
    </script>
</body>

</html>

